{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"authorship_tag":"ABX9TyMKBNv0DgMROlxH5KUikvBG"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11939197,"sourceType":"datasetVersion","datasetId":7314670}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Preparing the Dataset","metadata":{"id":"xAVF_OkQ4Bie"}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:21:03.367343Z","iopub.execute_input":"2025-08-30T03:21:03.367834Z","iopub.status.idle":"2025-08-30T03:21:03.372986Z","shell.execute_reply.started":"2025-08-30T03:21:03.367810Z","shell.execute_reply":"2025-08-30T03:21:03.372308Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/vietnamese-comment/comments.csv\n/kaggle/input/vietnamese-comment/vi_word2vec.txt\n/kaggle/input/vietnamese-comment/extra.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport re ","metadata":{"id":"C_70PqesOxnp","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:21:06.101330Z","iopub.execute_input":"2025-08-30T03:21:06.101598Z","iopub.status.idle":"2025-08-30T03:21:06.105388Z","shell.execute_reply.started":"2025-08-30T03:21:06.101579Z","shell.execute_reply":"2025-08-30T03:21:06.104739Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"data_path_1 = '/kaggle/input/vietnamese-comment/comments.csv'\ndata_path_2 = '/kaggle/input/vietnamese-comment/extra.csv' ","metadata":{"executionInfo":{"elapsed":17824,"status":"ok","timestamp":1746193805847,"user":{"displayName":"Viet Tranhuu","userId":"08791169309344190178"},"user_tz":-420},"id":"vDsAVM7jfp-F","outputId":"7fa7ea5d-243d-42dd-ba4d-0aa1065f2334","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:21:09.305030Z","iopub.execute_input":"2025-08-30T03:21:09.305762Z","iopub.status.idle":"2025-08-30T03:21:09.308981Z","shell.execute_reply.started":"2025-08-30T03:21:09.305737Z","shell.execute_reply":"2025-08-30T03:21:09.308324Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"data_1 = pd.read_csv(data_path_1)\ndata_1.head()","metadata":{"executionInfo":{"elapsed":2089,"status":"ok","timestamp":1746193810496,"user":{"displayName":"Viet Tranhuu","userId":"08791169309344190178"},"user_tz":-420},"id":"7YUIE0d3P4JD","outputId":"063eac9b-2064-4ead-9129-5f9eeb8c0039","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:21:11.811001Z","iopub.execute_input":"2025-08-30T03:21:11.811667Z","iopub.status.idle":"2025-08-30T03:21:12.313277Z","shell.execute_reply.started":"2025-08-30T03:21:11.811643Z","shell.execute_reply":"2025-08-30T03:21:12.312476Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                title                                            content  \\\n0     Cực kì hài lòng  Được hẹn giao hàng thứ 4 mà thứ 2 đã có sách t...   \n1  Rất không hài lòng  Hàng giả.  Giấy sách quá tệ. Chữ in ko rõ ràng...   \n2     Cực kì hài lòng  Sách đẹp, chất lượng giấy tuyệt vời, khổ to, n...   \n3     Cực kì hài lòng  Giao hàng nhanh, đóng gói cẩn thận. Có bóng kí...   \n4     Cực kì hài lòng        Giao nhanh và đóng gói cẩn thận. Thank shop   \n\n   rating  \n0       5  \n1       1  \n2       5  \n3       5  \n4       5  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>content</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Cực kì hài lòng</td>\n      <td>Được hẹn giao hàng thứ 4 mà thứ 2 đã có sách t...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Rất không hài lòng</td>\n      <td>Hàng giả.  Giấy sách quá tệ. Chữ in ko rõ ràng...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Cực kì hài lòng</td>\n      <td>Sách đẹp, chất lượng giấy tuyệt vời, khổ to, n...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Cực kì hài lòng</td>\n      <td>Giao hàng nhanh, đóng gói cẩn thận. Có bóng kí...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Cực kì hài lòng</td>\n      <td>Giao nhanh và đóng gói cẩn thận. Thank shop</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"len(data_1) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_2 = pd.read_csv(data_path_2)\ndata_2.head()","metadata":{"executionInfo":{"elapsed":43,"status":"ok","timestamp":1746193812178,"user":{"displayName":"Viet Tranhuu","userId":"08791169309344190178"},"user_tz":-420},"id":"uzkSZ0Shdzt6","outputId":"d9a7f01d-8250-4fdb-8119-f638504e4ddc","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(data_2) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# data_1 = data_1.drop(columns=['title'])\n# data = pd.concat([data_1, data_2])\n\ndata = data_1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:21:16.906605Z","iopub.execute_input":"2025-08-30T03:21:16.906879Z","iopub.status.idle":"2025-08-30T03:21:16.910417Z","shell.execute_reply.started":"2025-08-30T03:21:16.906856Z","shell.execute_reply":"2025-08-30T03:21:16.909707Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"data.head(10) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(data) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"missing_values = data.isnull().sum()\nprint(\"Dữ liệu bị thiếu:\\n\", missing_values)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"duplicates = data.duplicated(subset=[\"content\"]).sum()\nprint(\"Số câu trùng lặp:\", duplicates)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Data Preprocessing","metadata":{"id":"8ZYScp6ADQOX"}},{"cell_type":"markdown","source":"## 2.1 Data Cleaning","metadata":{"id":"LGaJuM4WD1eT"}},{"cell_type":"code","source":"# data = data.dropna(subset=[\"content\"])           # bỏ comment rỗng\ndata = data.dropna(subset=['title', 'content']) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:21:22.512696Z","iopub.execute_input":"2025-08-30T03:21:22.512967Z","iopub.status.idle":"2025-08-30T03:21:22.540136Z","shell.execute_reply.started":"2025-08-30T03:21:22.512948Z","shell.execute_reply":"2025-08-30T03:21:22.539343Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"len(data)","metadata":{"executionInfo":{"elapsed":39,"status":"ok","timestamp":1746193815494,"user":{"displayName":"Viet Tranhuu","userId":"08791169309344190178"},"user_tz":-420},"id":"6gcND-Kbd5nN","outputId":"d98cee72-c031-424a-eb30-43168952e83b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import unicodedata","metadata":{"id":"mdpDYHzWVnK_","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:21:26.984314Z","iopub.execute_input":"2025-08-30T03:21:26.984598Z","iopub.status.idle":"2025-08-30T03:21:26.988259Z","shell.execute_reply.started":"2025-08-30T03:21:26.984575Z","shell.execute_reply":"2025-08-30T03:21:26.987549Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Dictionary for common Vietnamese slang/abbreviations\nabbreviations = {\n    \"ko\": \"không\",\n    \"sp\": \"sản phẩm\",\n    \"k\": \"không\",\n    \"m\": \"mình\",\n    \"đc\": \"được\",\n    \"dc\": \"được\",\n    \"h\": \"giờ\",\n    \"trloi\": \"trả lời\",\n    \"cg\": \"cũng\",\n    \"bt\": \"bình thường\",\n    \"dt\": \"điện thoại\",\n    \"mt\": \"máy tính\",\n    \"m.n\": \"mọi người\"\n    # add more slang mappings\n}\n\n# Regex patterns\nurl_pattern = r\"http\\S+|www\\S+\"  # URLs\nuser_pattern = r\"@\\w+\"  # usernames\nemoji_pattern = re.compile(\n    \"[\"  # start\n    \"\\U0001F600-\\U0001F64F\"  # emoticons\n    \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n    \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n    \"\\U0001F1E0-\\U0001F1FF\"  # flags\n    \"]+\", flags=re.UNICODE)\nemoticon_pattern = r\"[:;=8][\\-o\\*']?[\\)\\]\\(\\[dDpP/:}\\{@\\|\\\\]\"  # emoticons\nrepeat_pattern = re.compile(r\"(.)\\1{2,}\")  # 3 or more repeats","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:21:29.837115Z","iopub.execute_input":"2025-08-30T03:21:29.837390Z","iopub.status.idle":"2025-08-30T03:21:29.842767Z","shell.execute_reply.started":"2025-08-30T03:21:29.837372Z","shell.execute_reply":"2025-08-30T03:21:29.841991Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def clean_text(text: str) -> str:\n    # Unicode normalization\n    text = str(text)\n    text = unicodedata.normalize('NFC', text)  # Chuẩn hoá Unicode rõ ràng (căn bản)\n\n    # Lowercase\n    text = text.lower()\n\n    # Remove URLs and usernames\n    text = re.sub(url_pattern, '', text)\n    text = re.sub(user_pattern, '', text)\n\n    # Remove emojis and emoticons\n    text = emoji_pattern.sub(' ', text)\n    text = re.sub(emoticon_pattern, ' ', text)\n\n    # Expand common abbreviations\n    def expand(match):\n        word = match.group(0)\n        return abbreviations.get(word, word)\n\n    if abbreviations:\n        pattern = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, abbreviations.keys())) + r\")\\b\")\n        text = pattern.sub(expand, text)\n\n    # Remove repeated characters (e.g., \"quaaa\" -> \"qua\" )\n    text = repeat_pattern.sub(r\"\\1\", text)\n\n    # Remove punctuation (keep Vietnamese letters & numbers)\n    text = re.sub(r\"[^\\w\\s\\u00C0-\\u024F]\", ' ', text)\n\n    # Remove extra whitespace\n    text = re.sub(r\"\\s+\", ' ', text).strip()\n\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:21:34.762117Z","iopub.execute_input":"2025-08-30T03:21:34.762655Z","iopub.status.idle":"2025-08-30T03:21:34.768362Z","shell.execute_reply.started":"2025-08-30T03:21:34.762632Z","shell.execute_reply":"2025-08-30T03:21:34.767612Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"sample = \"Tui thik qááá!!! 😊😊, thanks @ban http://example.com\"\nprint(clean_text(sample))  # Expected: \"tui thích qua cảm ơn\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data[\"text\"] = data[\"content\"].apply(clean_text)","metadata":{"id":"1hcGFVNdVqvk","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:21:40.247298Z","iopub.execute_input":"2025-08-30T03:21:40.247549Z","iopub.status.idle":"2025-08-30T03:21:43.496981Z","shell.execute_reply.started":"2025-08-30T03:21:40.247534Z","shell.execute_reply":"2025-08-30T03:21:43.495970Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_183/4067448078.py:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[\"text\"] = data[\"content\"].apply(clean_text)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"data.head()","metadata":{"executionInfo":{"elapsed":260,"status":"ok","timestamp":1746193819053,"user":{"displayName":"Viet Tranhuu","userId":"08791169309344190178"},"user_tz":-420},"id":"O2XJfW-dVtpQ","outputId":"55efdbe7-166a-4aec-c4b9-8e9fd1224ac0","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = data.groupby('text', as_index=False)['rating'].mean()  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:21:48.507161Z","iopub.execute_input":"2025-08-30T03:21:48.507827Z","iopub.status.idle":"2025-08-30T03:21:48.639637Z","shell.execute_reply.started":"2025-08-30T03:21:48.507806Z","shell.execute_reply":"2025-08-30T03:21:48.639103Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"data['rating'] = np.floor(data['rating']).astype(int) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:21:51.521871Z","iopub.execute_input":"2025-08-30T03:21:51.522586Z","iopub.status.idle":"2025-08-30T03:21:51.531386Z","shell.execute_reply.started":"2025-08-30T03:21:51.522562Z","shell.execute_reply":"2025-08-30T03:21:51.530413Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"duplicates = data.duplicated(subset=[\"text\"]).sum()\nprint(\"Số câu trùng lặp sau xử lý:\", duplicates) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:21:57.561300Z","iopub.execute_input":"2025-08-30T03:21:57.561793Z","iopub.status.idle":"2025-08-30T03:21:57.581717Z","shell.execute_reply.started":"2025-08-30T03:21:57.561769Z","shell.execute_reply":"2025-08-30T03:21:57.581203Z"}},"outputs":[{"name":"stdout","text":"Số câu trùng lặp sau xử lý: 0\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"def sentiment(r):\n    if r >= 4: return \"tích cực\"\n    if r == 3: return \"bình thường\"\n    return \"tiêu cực\"\ndef label(r):\n    if r >= 4: return 2\n    if r == 3: return 1\n    return 0\ndata[\"sentiment\"] = data[\"rating\"].apply(sentiment)\ndata[\"label\"] = data[\"rating\"].apply(label)","metadata":{"id":"vX1efgvuebd6","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:22:00.485189Z","iopub.execute_input":"2025-08-30T03:22:00.485434Z","iopub.status.idle":"2025-08-30T03:22:00.511633Z","shell.execute_reply.started":"2025-08-30T03:22:00.485418Z","shell.execute_reply":"2025-08-30T03:22:00.510901Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"data.head()","metadata":{"executionInfo":{"elapsed":266,"status":"ok","timestamp":1746193821607,"user":{"displayName":"Viet Tranhuu","userId":"08791169309344190178"},"user_tz":-420},"id":"5iVRzBnqeghM","outputId":"2398a97d-ce17-45c1-d429-250f9296dea6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.2 Statistical Analysis","metadata":{"id":"tGRmgNK5D89Q"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"id":"gv976yWHWcaA","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:22:04.216355Z","iopub.execute_input":"2025-08-30T03:22:04.217074Z","iopub.status.idle":"2025-08-30T03:22:04.782159Z","shell.execute_reply.started":"2025-08-30T03:22:04.217047Z","shell.execute_reply":"2025-08-30T03:22:04.781597Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"print(\"Số lượng mẫu:\", data.shape[0])","metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1746193825497,"user":{"displayName":"Viet Tranhuu","userId":"08791169309344190178"},"user_tz":-420},"id":"VwBOfngAWf4D","outputId":"d4863a13-a29d-46ef-9ec5-776935629c8e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(6, 4))\nsns.countplot(x=data[\"sentiment\"], palette=\"coolwarm\")\nplt.title(\"Phân phối nhãn cảm xúc\")\nplt.xlabel(\"Cảm xúc\")\nplt.ylabel(\"Số lượng mẫu\")\nplt.show()","metadata":{"executionInfo":{"elapsed":461,"status":"ok","timestamp":1746193827036,"user":{"displayName":"Viet Tranhuu","userId":"08791169309344190178"},"user_tz":-420},"id":"iJG9XwFVWlub","outputId":"47ce09af-04be-4643-df07-c2e9975570b1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"missing_values = data.isnull().sum()\nprint(\"Dữ liệu bị thiếu:\\n\", missing_values)","metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1746193828233,"user":{"displayName":"Viet Tranhuu","userId":"08791169309344190178"},"user_tz":-420},"id":"j3x1DPRXWpj8","outputId":"d3cf5b28-3c0e-4d11-8a1f-2a5251d9761e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label_counts = data[\"sentiment\"].value_counts()\nprint(\"Số lượng mỗi nhãn:\\n\", label_counts)","metadata":{"executionInfo":{"elapsed":42,"status":"ok","timestamp":1746193831015,"user":{"displayName":"Viet Tranhuu","userId":"08791169309344190178"},"user_tz":-420},"id":"gvWYi5LLXZSO","outputId":"ee01d7ee-aea6-4c5c-92c4-7382a5289079","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install underthesea ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:22:15.353309Z","iopub.execute_input":"2025-08-30T03:22:15.353718Z","iopub.status.idle":"2025-08-30T03:22:18.383066Z","shell.execute_reply.started":"2025-08-30T03:22:15.353698Z","shell.execute_reply":"2025-08-30T03:22:18.382337Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: underthesea in /usr/local/lib/python3.11/dist-packages (6.8.4)\nRequirement already satisfied: Click>=6.0 in /usr/local/lib/python3.11/dist-packages (from underthesea) (8.1.8)\nRequirement already satisfied: python-crfsuite>=0.9.6 in /usr/local/lib/python3.11/dist-packages (from underthesea) (0.9.11)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from underthesea) (3.9.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from underthesea) (4.67.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from underthesea) (2.32.3)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from underthesea) (1.4.2)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from underthesea) (1.2.2)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from underthesea) (6.0.2)\nRequirement already satisfied: underthesea-core==1.0.4 in /usr/local/lib/python3.11/dist-packages (from underthesea) (1.0.4)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->underthesea) (2024.11.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->underthesea) (2025.1.31)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->underthesea) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->underthesea) (1.15.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->underthesea) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->underthesea) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->underthesea) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->underthesea) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->underthesea) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->underthesea) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn->underthesea) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn->underthesea) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn->underthesea) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn->underthesea) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn->underthesea) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->scikit-learn->underthesea) (2024.2.0)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"from underthesea import word_tokenize\nimport nltk\nimport wordcloud","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:22:39.991406Z","iopub.execute_input":"2025-08-30T03:22:39.992078Z","iopub.status.idle":"2025-08-30T03:22:44.962230Z","shell.execute_reply.started":"2025-08-30T03:22:39.992047Z","shell.execute_reply":"2025-08-30T03:22:44.961401Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"data['corpus'] = data['text'].map(lambda text: word_tokenize(text, format=\"text\")) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:22:47.152706Z","iopub.execute_input":"2025-08-30T03:22:47.153212Z","iopub.status.idle":"2025-08-30T03:23:57.484831Z","shell.execute_reply.started":"2025-08-30T03:22:47.153189Z","shell.execute_reply":"2025-08-30T03:23:57.484212Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"data.sample(10) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create bag of words\n# Flatten the list of lists into a single list of words\nall_words_flat = []\nfor tokens in data['corpus'].tolist():\n    if tokens and tokens != '':\n        all_words_flat.extend(tokens.split())\n\n# Create FreqDist from the flattened list\nall_words_dist = nltk.FreqDist(all_words_flat)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print the total number of words and the 15 most common words\nprint('Tổng số từ: {}'.format(len(all_words_dist)))\nprint('Từ xuất hiện nhiều: {}'.format(all_words_dist.most_common(15)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"corpus = \" \".join(all_words_flat)\nplt.figure(figsize=(12,8))\nword_cloud = wordcloud.WordCloud(max_words=100, background_color =\"black\", width=2000, height=1000, mode=\"RGB\").generate(corpus)\nplt.axis(\"off\")\nplt.imshow(word_cloud)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2.3 Oversampling","metadata":{"id":"_b-MYlkuEJoD"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"id":"L8qIy5ekB4I9","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:24:03.272299Z","iopub.execute_input":"2025-08-30T03:24:03.272561Z","iopub.status.idle":"2025-08-30T03:24:03.276172Z","shell.execute_reply.started":"2025-08-30T03:24:03.272543Z","shell.execute_reply":"2025-08-30T03:24:03.275561Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"train_sentences, test_sentences, train_labels, test_labels = train_test_split(\n    data['text'],\n    data['label'],\n    test_size=0.2,\n    stratify=data['label'],\n    random_state=42\n)","metadata":{"id":"eeAd3GCb-GkT","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:24:05.818715Z","iopub.execute_input":"2025-08-30T03:24:05.819271Z","iopub.status.idle":"2025-08-30T03:24:05.847337Z","shell.execute_reply.started":"2025-08-30T03:24:05.819249Z","shell.execute_reply":"2025-08-30T03:24:05.846753Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"from sklearn.utils import resample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:24:08.737037Z","iopub.execute_input":"2025-08-30T03:24:08.737756Z","iopub.status.idle":"2025-08-30T03:24:08.740995Z","shell.execute_reply.started":"2025-08-30T03:24:08.737731Z","shell.execute_reply":"2025-08-30T03:24:08.740443Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"df_train = pd.DataFrame({\n    'text': train_sentences,\n    'label': train_labels\n})\n\ndf_pos = df_train[df_train.label == 2]   # positive\ndf_neg = df_train[df_train.label == 0]   # negative\ndf_neu = df_train[df_train.label == 1]   # neutral\n\nmax_n = df_train.label.value_counts().max()\n\ndf_neg_up = resample(df_neg,\n                     replace=True,\n                     n_samples=max_n,\n                     random_state=42)\ndf_neu_up = resample(df_neu,\n                     replace=True,\n                     n_samples=max_n,\n                     random_state=42)\n\ndf_pos_up = df_pos\n\ntrain_balanced = pd.concat([df_pos_up, df_neg_up, df_neu_up])\ntrain_balanced = train_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n\ntrain_sentences = train_balanced['text']\ntrain_labels    = train_balanced['label']","metadata":{"executionInfo":{"elapsed":49,"status":"ok","timestamp":1746193834633,"user":{"displayName":"Viet Tranhuu","userId":"08791169309344190178"},"user_tz":-420},"id":"zQQTBvoXAIH0","outputId":"75785bc9-a527-4422-dc0b-0c1145c1103c","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:24:10.921678Z","iopub.execute_input":"2025-08-30T03:24:10.922214Z","iopub.status.idle":"2025-08-30T03:24:10.951002Z","shell.execute_reply.started":"2025-08-30T03:24:10.922189Z","shell.execute_reply":"2025-08-30T03:24:10.950498Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"print(train_balanced['label'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:24:14.073142Z","iopub.execute_input":"2025-08-30T03:24:14.073727Z","iopub.status.idle":"2025-08-30T03:24:14.079382Z","shell.execute_reply.started":"2025-08-30T03:24:14.073703Z","shell.execute_reply":"2025-08-30T03:24:14.078473Z"}},"outputs":[{"name":"stdout","text":"label\n0    27976\n2    27976\n1    27976\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"# 3. ML Model","metadata":{"id":"rd_WHu8v4Khn"}},{"cell_type":"markdown","source":"## 3.1 Multinomial Naive Bayes","metadata":{"id":"PkTBTlCX3BVW"}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix","metadata":{"id":"Son705hwMCiM","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vectorizer = TfidfVectorizer()\n\ntrain_sentences_tfidf = vectorizer.fit_transform(train_sentences)\ntest_sentences_tfidf = vectorizer.transform(test_sentences)","metadata":{"id":"sp40BtkkMP_o","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = MultinomialNB()\nmodel.fit(train_sentences_tfidf, train_labels)","metadata":{"executionInfo":{"elapsed":45,"status":"ok","timestamp":1746180731487,"user":{"displayName":"Viet Tranhuu","userId":"08791169309344190178"},"user_tz":-420},"id":"KSIB1HwdNlgX","outputId":"5475d948-2f88-42d1-dfef-af5bd5614bb5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pred = model.predict(test_sentences_tfidf)","metadata":{"id":"wBL3a8piNmYl","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Accuracy:\", accuracy_score(test_labels, pred))\nprint(\"Classification Report:\")\nprint(classification_report(test_labels, pred))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(test_labels, pred))","metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1746180734935,"user":{"displayName":"Viet Tranhuu","userId":"08791169309344190178"},"user_tz":-420},"id":"-ht0JEjoNqgK","outputId":"3d4445cd-f5c6-4f04-918f-bf9ffc0b85c3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"label2sen = {\n    0: \"Tiêu cực\",\n    1: \"Bình thường\",\n    2: \"Tích cực\"\n}\nwhile True:\n  input_text = input(\"Nhập câu cần kiểm tra: \")\n  if input_text.strip().lower() != \"thoát\":\n    input_text_tfidf = vectorizer.transform([clean_text(input_text)])\n    prediction = model.predict(input_text_tfidf)\n    print(\"Kết quả dự đoán: \" + label2sen[prediction[0]] + \"\\n\")\n  else:\n    print(\"Chúc một ngày tốt lành !\")\n    break","metadata":{"executionInfo":{"elapsed":76177,"status":"ok","timestamp":1746174096531,"user":{"displayName":"Viet Tranhuu","userId":"08791169309344190178"},"user_tz":-420},"id":"e_e0rRMPULmq","outputId":"ecf8cbaa-26d3-491d-a6bc-cc3c681b63fc","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. DL Model","metadata":{"id":"y4WZDW5wu9y2"}},{"cell_type":"markdown","source":"## 4.1 Word Embedding","metadata":{"id":"-uvILOwq_SbJ"}},{"cell_type":"code","source":"!pip install torch==2.2.0 ","metadata":{"executionInfo":{"elapsed":176883,"status":"ok","timestamp":1746194015784,"user":{"displayName":"Viet Tranhuu","userId":"08791169309344190178"},"user_tz":-420},"id":"NRwtkFQ_BMNt","outputId":"74c332cc-5ead-45a3-8e0f-cd3723c0f59e","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:24:27.840477Z","iopub.execute_input":"2025-08-30T03:24:27.841049Z","iopub.status.idle":"2025-08-30T03:24:30.769000Z","shell.execute_reply.started":"2025-08-30T03:24:27.841017Z","shell.execute_reply":"2025-08-30T03:24:30.768257Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch==2.2.0 in /usr/local/lib/python3.11/dist-packages (2.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (4.13.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (1.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (2.19.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (12.1.105)\nRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0) (2.2.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0) (12.8.93)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.0) (3.0.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.0) (1.3.0)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"!pip install torchtext==0.17.0 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:24:35.907368Z","iopub.execute_input":"2025-08-30T03:24:35.908209Z","iopub.status.idle":"2025-08-30T03:24:38.952107Z","shell.execute_reply.started":"2025-08-30T03:24:35.908173Z","shell.execute_reply":"2025-08-30T03:24:38.951142Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torchtext==0.17.0 in /usr/local/lib/python3.11/dist-packages (0.17.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.0) (4.67.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.0) (2.32.3)\nRequirement already satisfied: torch==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.0) (2.2.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.0) (1.26.4)\nRequirement already satisfied: torchdata==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.0) (0.7.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchtext==0.17.0) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchtext==0.17.0) (4.13.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchtext==0.17.0) (1.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchtext==0.17.0) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchtext==0.17.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchtext==0.17.0) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchtext==0.17.0) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchtext==0.17.0) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchtext==0.17.0) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchtext==0.17.0) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchtext==0.17.0) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchtext==0.17.0) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchtext==0.17.0) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchtext==0.17.0) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchtext==0.17.0) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchtext==0.17.0) (2.19.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchtext==0.17.0) (12.1.105)\nRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->torchtext==0.17.0) (2.2.0)\nRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.7.1->torchtext==0.17.0) (2.3.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0->torchtext==0.17.0) (12.8.93)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchtext==0.17.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchtext==0.17.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchtext==0.17.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchtext==0.17.0) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchtext==0.17.0) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchtext==0.17.0) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.0) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.0) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.0) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.0->torchtext==0.17.0) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchtext==0.17.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchtext==0.17.0) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchtext==0.17.0) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchtext==0.17.0) (2024.2.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.0->torchtext==0.17.0) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchtext==0.17.0) (2024.2.0)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import torch \nimport torchtext.vocab as vocab","metadata":{"executionInfo":{"elapsed":4776,"status":"ok","timestamp":1746194050798,"user":{"displayName":"Viet Tranhuu","userId":"08791169309344190178"},"user_tz":-420},"id":"KlSyq9yz_ZLe","outputId":"3e4fce0d-2897-4f0b-ad0b-cf101ad85ba1","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:24:43.359688Z","iopub.execute_input":"2025-08-30T03:24:43.360007Z","iopub.status.idle":"2025-08-30T03:24:44.525720Z","shell.execute_reply.started":"2025-08-30T03:24:43.359979Z","shell.execute_reply":"2025-08-30T03:24:44.525131Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# input_path = '/kaggle/input/vietnamese-comment/vi_word2vec.txt'\n# output_path = '/kaggle/working/vi_word2vec_reduced.txt' \n# max_lines = 100000  # Số dòng bạn muốn giữ lại ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:25:02.303710Z","iopub.execute_input":"2025-08-30T03:25:02.304693Z","iopub.status.idle":"2025-08-30T03:25:02.308420Z","shell.execute_reply.started":"2025-08-30T03:25:02.304665Z","shell.execute_reply":"2025-08-30T03:25:02.307617Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"# with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8') as outfile:\n#     for i, line in enumerate(infile):\n#         if i > max_lines:\n#             break\n#         outfile.write(line) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:25:24.740304Z","iopub.execute_input":"2025-08-30T03:25:24.740856Z","iopub.status.idle":"2025-08-30T03:25:26.060234Z","shell.execute_reply.started":"2025-08-30T03:25:24.740829Z","shell.execute_reply":"2025-08-30T03:25:26.059537Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"word_embedding = vocab.Vectors(name = '/kaggle/working/vi_word2vec_reduced.txt', unk_init = torch.Tensor.normal_)\nword_embedding.vectors.shape","metadata":{"executionInfo":{"elapsed":103600,"status":"ok","timestamp":1746194158170,"user":{"displayName":"Viet Tranhuu","userId":"08791169309344190178"},"user_tz":-420},"id":"XNwfsCmxBk3K","outputId":"2d0c240f-d4d4-48c2-ffd9-c1706a04a6c5","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:25:33.959428Z","iopub.execute_input":"2025-08-30T03:25:33.960002Z","iopub.status.idle":"2025-08-30T03:25:38.597101Z","shell.execute_reply.started":"2025-08-30T03:25:33.959977Z","shell.execute_reply":"2025-08-30T03:25:38.596466Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 100000/100000 [00:04<00:00, 23040.19it/s]\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"torch.Size([100000, 100])"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"def get_vector(embeddings, word):\n    assert word in embeddings.stoi, f'*{word}* is not in the vocab!'\n    return embeddings.vectors[embeddings.stoi[word]]\n\ndef closest_words(embeddings, vector, n=10):\n    distances = [(word, torch.dist(vector, get_vector(embeddings, word)).item())\n                 for word in embeddings.itos]\n\n    return sorted(distances, key = lambda w: w[1])[:n]","metadata":{"executionInfo":{"elapsed":15471,"status":"ok","timestamp":1746194194823,"user":{"displayName":"Viet Tranhuu","userId":"08791169309344190178"},"user_tz":-420},"id":"Ug_yYagDIxD3","outputId":"e1cf4730-d8c1-4c4a-e0a4-6824f3ce1eee","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:25:41.859598Z","iopub.execute_input":"2025-08-30T03:25:41.860230Z","iopub.status.idle":"2025-08-30T03:25:41.865165Z","shell.execute_reply.started":"2025-08-30T03:25:41.860205Z","shell.execute_reply":"2025-08-30T03:25:41.864369Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"word_vector = get_vector(word_embedding, \"Lạc_Long_Quân\")\n\nclosest_words(word_embedding, word_vector, n=20)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:25:45.879893Z","iopub.execute_input":"2025-08-30T03:25:45.880604Z","iopub.status.idle":"2025-08-30T03:25:46.747060Z","shell.execute_reply.started":"2025-08-30T03:25:45.880578Z","shell.execute_reply":"2025-08-30T03:25:46.746449Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"[('Lạc_Long_Quân', 0.0),\n ('Âu_Cơ', 0.44687357544898987),\n ('An_Dương_Vương', 0.5421355962753296),\n ('Hoàng_Hoa_Thám', 0.5765705704689026),\n ('Thuỵ_Khuê', 0.6254850029945374),\n ('Lê_Đại_Hành', 0.6597759127616882),\n ('Nguyễn_Hoàng_Tôn', 0.6894202828407288),\n ('Lý_Thái_Tổ', 0.690030574798584),\n ('Xuân_La', 0.6982914805412292),\n ('Nguyễn_Trãi', 0.7008039355278015),\n ('Kinh_Dương_Vương', 0.7009497880935669),\n ('Hoàng_Văn_Thụ', 0.7065233588218689),\n ('Trương_Định', 0.71026211977005),\n ('Nghi_Tàm', 0.711430013179779),\n ('Tây_Hồ', 0.714965283870697),\n ('Liễu_Giai', 0.7252614498138428),\n ('Thuỵ_Khê', 0.7305408716201782),\n ('Nguyễn_Văn_Cừ', 0.7348453402519226),\n ('Lý_Thường_Kiệt', 0.7370755672454834),\n ('Linh_Lang', 0.7411515712738037)]"},"metadata":{}}],"execution_count":32},{"cell_type":"markdown","source":"## 4.2 Vocabulary Class","metadata":{"id":"kPhuKPoCKP4Y"}},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"id":"TUzIsdjUI8Yv","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:25:51.254973Z","iopub.execute_input":"2025-08-30T03:25:51.255577Z","iopub.status.idle":"2025-08-30T03:25:51.259001Z","shell.execute_reply.started":"2025-08-30T03:25:51.255554Z","shell.execute_reply":"2025-08-30T03:25:51.258298Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"class Vocabulary:\n    def __init__(self):\n        self.word2id = dict()\n        self.word2id['<pad>'] = 0   # Pad Token\n        self.word2id['<unk>'] = 1   # Unknown Token\n        self.unk_id = self.word2id['<unk>']\n        self.id2word = {v: k for k, v in self.word2id.items()}\n\n    def __getitem__(self, word):\n        return self.word2id.get(word, self.unk_id)\n\n    def __contains__(self, word):\n        return word in self.word2id\n\n    def __len__(self):\n        return len(self.word2id)\n\n    def id2word(self, word_index):\n        return self.id2word[word_index]\n\n    def add(self, word):\n        if word not in self:\n            word_index = self.word2id[word] = len(self.word2id)\n            self.id2word[word_index] = word\n            return word_index\n        else:\n            return self[word]\n\n    @staticmethod\n    def tokenize_corpus(corpus):\n        print(\"Tokenize the corpus...\")\n        tokenized_corpus = list()\n        for document in tqdm(corpus):\n            tokenized_document = [word.replace(\" \", \"_\") for word in word_tokenize(document)]\n            tokenized_corpus.append(tokenized_document)\n\n        return tokenized_corpus\n\n    def corpus_to_tensor(self, corpus, is_tokenized=False):\n        if is_tokenized:\n            tokenized_corpus = corpus\n        else:\n            tokenized_corpus = self.tokenize_corpus(corpus)\n        indicies_corpus = list()\n        for document in tqdm(tokenized_corpus):\n            indicies_document = torch.tensor(list(map(lambda word: self[word], document)),\n                                             dtype=torch.int64)\n            indicies_corpus.append(indicies_document)\n\n        return indicies_corpus\n\n    def tensor_to_corpus(self, tensor):\n        corpus = list()\n        for indicies in tqdm(tensor):\n            document = list(map(lambda index: self.id2word[index.item()], indicies))\n            corpus.append(document)\n\n        return corpus\n\n    # def add_words_from_corpus(self, corpus, is_tokenized=False):\n    #     print(\"Add words from the corpus...\")\n    #     if is_tokenized:\n    #         tokenized_corpus = corpus\n    #     else:\n    #         tokenized_corpus = self.tokenize_corpus(corpus)\n    #     word_freq = Counter(chain(*tokenized_corpus))\n    #     non_singletons = [w for w in word_freq if word_freq[w] > 1]\n    #     print(f\"Number of words in the corpus: {len(word_freq)}\")\n    #     print(f\"Number of words with frequency > 1: {len(non_singletons)}\")\n    #     for word in non_singletons:\n    #         self.add(word)","metadata":{"id":"y0OKmH59JRNR","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:26:27.682526Z","iopub.execute_input":"2025-08-30T03:26:27.682826Z","iopub.status.idle":"2025-08-30T03:26:27.691375Z","shell.execute_reply.started":"2025-08-30T03:26:27.682804Z","shell.execute_reply":"2025-08-30T03:26:27.690556Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"corpus_sample = [\"Đẹp lắm mn ơi k ngờ fahasa bán alb thơ này của Lana lun, bh khó mua lắm\",\n                 \"Shop giao hàng nhanh, đóng gói hàng cẩn thận. Mặc dù sách có bé hơn mình nghĩ nhưng shop rất chu đáo. Vì mình mua gần tết nên có đc tặng thêm cả lì xì nữa. Rất đáng tiền. Mn mua ủng hộ shop nhé.\",\n                 \"lần đầu mua nhưng ok lắm luôn sắp tết nên đc tặng tập lì xì sách nhỏ nhưng bọc hộp đầy đủ đặc biệt tặng cả voucher cho lần sau chỉ có cái sách được bọc bằng màng thực phẩm\"]\n\nVocabulary.tokenize_corpus(corpus_sample)","metadata":{"executionInfo":{"elapsed":753,"status":"ok","timestamp":1746194235139,"user":{"displayName":"Viet Tranhuu","userId":"08791169309344190178"},"user_tz":-420},"id":"qPuoNxT8Jssh","outputId":"60fa012f-5e6c-4213-fa79-da45b15fd327","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:26:30.836416Z","iopub.execute_input":"2025-08-30T03:26:30.837159Z","iopub.status.idle":"2025-08-30T03:26:30.854390Z","shell.execute_reply.started":"2025-08-30T03:26:30.837126Z","shell.execute_reply":"2025-08-30T03:26:30.853622Z"}},"outputs":[{"name":"stdout","text":"Tokenize the corpus...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3/3 [00:00<00:00, 382.40it/s]\n","output_type":"stream"},{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"[['Đẹp',\n  'lắm',\n  'mn',\n  'ơi',\n  'k',\n  'ngờ',\n  'fahasa',\n  'bán',\n  'alb_thơ',\n  'này',\n  'của',\n  'Lana_lun',\n  ',',\n  'bh',\n  'khó',\n  'mua',\n  'lắm'],\n ['Shop',\n  'giao',\n  'hàng',\n  'nhanh',\n  ',',\n  'đóng_gói',\n  'hàng',\n  'cẩn_thận',\n  '.',\n  'Mặc_dù',\n  'sách',\n  'có',\n  'bé',\n  'hơn',\n  'mình',\n  'nghĩ',\n  'nhưng',\n  'shop',\n  'rất',\n  'chu_đáo',\n  '.',\n  'Vì',\n  'mình',\n  'mua',\n  'gần',\n  'tết',\n  'nên',\n  'có',\n  'đc',\n  'tặng',\n  'thêm',\n  'cả',\n  'lì_xì',\n  'nữa',\n  '.',\n  'Rất',\n  'đáng',\n  'tiền',\n  '.',\n  'Mn',\n  'mua',\n  'ủng_hộ',\n  'shop',\n  'nhé',\n  '.'],\n ['lần',\n  'đầu',\n  'mua',\n  'nhưng',\n  'ok',\n  'lắm',\n  'luôn',\n  'sắp',\n  'tết',\n  'nên',\n  'đc',\n  'tặng',\n  'tập_lì',\n  'xì',\n  'sách',\n  'nhỏ',\n  'nhưng',\n  'bọc_hộp',\n  'đầy_đủ',\n  'đặc_biệt',\n  'tặng',\n  'cả',\n  'voucher',\n  'cho',\n  'lần',\n  'sau',\n  'chỉ',\n  'có',\n  'cái',\n  'sách',\n  'được',\n  'bọc',\n  'bằng',\n  'màng',\n  'thực_phẩm']]"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"vocab = Vocabulary()\n\n# create vocabulary from pretrained word2vec\nwords_list = list(word_embedding.stoi.keys())\nfor word in words_list:\n    vocab.add(word)\n\n# test the vocabulary\ntensor = vocab.corpus_to_tensor(corpus_sample)\ncorpus = vocab.tensor_to_corpus(tensor)\n\" \".join(corpus[0])","metadata":{"executionInfo":{"elapsed":1427,"status":"ok","timestamp":1746194239758,"user":{"displayName":"Viet Tranhuu","userId":"08791169309344190178"},"user_tz":-420},"id":"CN5owFxXJ0ad","outputId":"588e75fb-406f-4322-dea2-c730afa3b7d3","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:26:35.592670Z","iopub.execute_input":"2025-08-30T03:26:35.593273Z","iopub.status.idle":"2025-08-30T03:26:35.671113Z","shell.execute_reply.started":"2025-08-30T03:26:35.593249Z","shell.execute_reply":"2025-08-30T03:26:35.670504Z"}},"outputs":[{"name":"stdout","text":"Tokenize the corpus...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 3/3 [00:00<00:00, 500.31it/s]\n100%|██████████| 3/3 [00:00<00:00, 12658.87it/s]\n100%|██████████| 3/3 [00:00<00:00, 3850.34it/s]\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"'Đẹp lắm <unk> ơi k ngờ <unk> bán <unk> này của <unk> , <unk> khó mua lắm'"},"metadata":{}}],"execution_count":36},{"cell_type":"markdown","source":"## 4.3 CommentDataset Class","metadata":{"id":"AUU-I4_4KgNQ"}},{"cell_type":"code","source":"from scipy.linalg.special_matrices import dft\nfrom torch.utils.data import Dataset","metadata":{"executionInfo":{"elapsed":29,"status":"ok","timestamp":1746194243323,"user":{"displayName":"Viet Tranhuu","userId":"08791169309344190178"},"user_tz":-420},"id":"ANHI2NEuKB7U","outputId":"7240a3da-1e92-40be-9c1e-ad707d4baa3a","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:26:38.728391Z","iopub.execute_input":"2025-08-30T03:26:38.728675Z","iopub.status.idle":"2025-08-30T03:26:38.733198Z","shell.execute_reply.started":"2025-08-30T03:26:38.728653Z","shell.execute_reply":"2025-08-30T03:26:38.732454Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_183/4042193658.py:1: DeprecationWarning: Please import `dft` from the `scipy.linalg` namespace; the `scipy.linalg.special_matrices` namespace is deprecated and will be removed in SciPy 2.0.0.\n  from scipy.linalg.special_matrices import dft\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"class CommentDataset(Dataset):\n\n    def __init__(self, vocab, df, tokenized_fpath=None):\n        self.vocab = vocab\n        self.pad_idx = vocab[\"<pad>\"]\n        df = df\n        self.sentiments_list = list(df.label)\n        self.reviews_list = list(df.text)\n\n        sentiments_type = list(set(self.sentiments_list))\n        sentiments_type.sort()\n\n        self.sentiment2id = {sentiment: i for i, sentiment in enumerate(sentiments_type)}\n\n        if tokenized_fpath:\n            self.tokenized_reviews = torch.load(tokenized_fpath)\n        else:\n            self.tokenized_reviews = self.vocab.tokenize_corpus(self.reviews_list)\n\n        self.tensor_data = self.vocab.corpus_to_tensor(self.tokenized_reviews, is_tokenized=True)\n        self.tensor_label = torch.tensor([self.sentiment2id[sentiment] for sentiment in self.sentiments_list],\n                                         dtype=torch.float64)\n\n        self.tensor_data, self.tensor_label = zip(*[(data, label) for data, label in zip(self.tensor_data, self.tensor_label) if len(data) > 0])\n        self.tensor_data = list(self.tensor_data)\n        self.tensor_label = torch.tensor(self.tensor_label, dtype=torch.float64) # Convert back to tensor\n\n    def __len__(self):\n        return len(self.tensor_data)\n\n    def __getitem__(self, idx):\n        return self.tensor_data[idx], self.tensor_label[idx]\n\n    def collate_fn(self, examples):\n        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n\n        reviews = [e[0] for e in examples]\n        reviews = torch.nn.utils.rnn.pad_sequence(reviews,\n                                                  batch_first=False,\n                                                  padding_value=self.pad_idx)\n        reviews_lengths = torch.tensor([len(e[0]) for e in examples])\n        sentiments = torch.tensor([e[1] for e in examples])\n\n        return {\"reviews\": (reviews, reviews_lengths), \"sentiments\": sentiments}","metadata":{"id":"kuT630Y0KvBm","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:26:52.265516Z","iopub.execute_input":"2025-08-30T03:26:52.266251Z","iopub.status.idle":"2025-08-30T03:26:52.274102Z","shell.execute_reply.started":"2025-08-30T03:26:52.266225Z","shell.execute_reply":"2025-08-30T03:26:52.273265Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"valid_df = train_balanced.sample(frac=0.2, random_state=42).reset_index()\ntrain_df = train_balanced.drop(valid_df.index).reset_index()\ntest_df = pd.DataFrame({\n    'text': test_sentences,\n    'label': test_labels\n}).reset_index()","metadata":{"id":"ceP8RzYJLSbF","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:26:55.589626Z","iopub.execute_input":"2025-08-30T03:26:55.590254Z","iopub.status.idle":"2025-08-30T03:26:55.620948Z","shell.execute_reply.started":"2025-08-30T03:26:55.590229Z","shell.execute_reply":"2025-08-30T03:26:55.620380Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"valid_df['label'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:27:00.756876Z","iopub.execute_input":"2025-08-30T03:27:00.757161Z","iopub.status.idle":"2025-08-30T03:27:00.763960Z","shell.execute_reply.started":"2025-08-30T03:27:00.757142Z","shell.execute_reply":"2025-08-30T03:27:00.763332Z"}},"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"label\n0    5644\n2    5588\n1    5554\nName: count, dtype: int64"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":" valid_df.drop(columns=['index'], inplace=True)\n train_df.drop(columns=['index'], inplace=True)\n test_df.drop(columns=['index'], inplace=True)","metadata":{"id":"yXs1JIFeL5V9","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:27:04.666366Z","iopub.execute_input":"2025-08-30T03:27:04.666649Z","iopub.status.idle":"2025-08-30T03:27:04.681967Z","shell.execute_reply.started":"2025-08-30T03:27:04.666628Z","shell.execute_reply":"2025-08-30T03:27:04.681136Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"train_dataset = CommentDataset(vocab, train_df)\nvalid_dataset = CommentDataset(vocab, valid_df)\ntest_dataset = CommentDataset(vocab, test_df)","metadata":{"executionInfo":{"elapsed":136089,"status":"ok","timestamp":1746194391233,"user":{"displayName":"Viet Tranhuu","userId":"08791169309344190178"},"user_tz":-420},"id":"bm8J8t-XMGUW","outputId":"6317c80d-6e3d-4d86-c908-544bb9f8b60b","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:27:07.051233Z","iopub.execute_input":"2025-08-30T03:27:07.051510Z","iopub.status.idle":"2025-08-30T03:29:50.261887Z","shell.execute_reply.started":"2025-08-30T03:27:07.051491Z","shell.execute_reply":"2025-08-30T03:29:50.261108Z"}},"outputs":[{"name":"stdout","text":"Tokenize the corpus...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 67142/67142 [01:52<00:00, 596.02it/s]\n100%|██████████| 67142/67142 [00:01<00:00, 52349.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Tokenize the corpus...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 16786/16786 [00:31<00:00, 535.20it/s]\n100%|██████████| 16786/16786 [00:00<00:00, 69039.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Tokenize the corpus...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 8756/8756 [00:16<00:00, 527.51it/s]\n100%|██████████| 8756/8756 [00:00<00:00, 63752.27it/s]\n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"## 4.4 Create DataLoader from IMDBDataset","metadata":{"id":"0dJyD-NkN6qR"}},{"cell_type":"code","source":"from torch.utils.data import DataLoader","metadata":{"id":"ok2-GZhwNvWs","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:30:09.683193Z","iopub.execute_input":"2025-08-30T03:30:09.683463Z","iopub.status.idle":"2025-08-30T03:30:09.686911Z","shell.execute_reply.started":"2025-08-30T03:30:09.683446Z","shell.execute_reply":"2025-08-30T03:30:09.686339Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"batch_size = 32\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, collate_fn=train_dataset.collate_fn)\nvalid_dataloader = DataLoader(valid_dataset, shuffle=True, batch_size=batch_size, collate_fn=valid_dataset.collate_fn)\ntest_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size, collate_fn=test_dataset.collate_fn)","metadata":{"id":"Ry9tY3fVOFR9","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:30:12.512193Z","iopub.execute_input":"2025-08-30T03:30:12.512479Z","iopub.status.idle":"2025-08-30T03:30:12.517608Z","shell.execute_reply.started":"2025-08-30T03:30:12.512458Z","shell.execute_reply":"2025-08-30T03:30:12.516692Z"}},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":"## 4.5 RNN Model","metadata":{"id":"DvsY8fMg3KUY"}},{"cell_type":"code","source":"import torch.nn as nn","metadata":{"id":"sDEYcDrgOHe5","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:30:15.898907Z","iopub.execute_input":"2025-08-30T03:30:15.899458Z","iopub.status.idle":"2025-08-30T03:30:15.902807Z","shell.execute_reply.started":"2025-08-30T03:30:15.899433Z","shell.execute_reply":"2025-08-30T03:30:15.902252Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"class RNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, n_layers,\n                 bidirectional, dropout, pad_idx, n_classes):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n        self.rnn = nn.LSTM(\n            embedding_dim,\n            hidden_dim,\n            num_layers=n_layers,\n            bidirectional=bidirectional,\n            dropout=dropout if n_layers > 1 else 0\n        )\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), n_classes)\n\n    def forward(self, text, text_lengths):\n        embedded = self.dropout(self.embedding(text))\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n            embedded, text_lengths.to('cpu'), enforce_sorted=False\n        )\n        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n        if self.rnn.bidirectional:\n            hidden = self.dropout(torch.cat((hidden[-2], hidden[-1]), dim=1))\n        else:\n            hidden = self.dropout(hidden[-1])\n        return self.fc(hidden)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:30:19.182615Z","iopub.execute_input":"2025-08-30T03:30:19.182902Z","iopub.status.idle":"2025-08-30T03:30:19.189611Z","shell.execute_reply.started":"2025-08-30T03:30:19.182882Z","shell.execute_reply":"2025-08-30T03:30:19.188818Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"input_dim = word_embedding.vectors.shape[0] \nembedding_dim = 100\nhidden_dim = 8  \nn_layers = 2\nbidirectional = False \ndropout = 0.3 \npad_idx = vocab[\"<pad>\"]\nunk_idx = vocab[\"<unk>\"]\nn_classes = 3  # positive, neutral, negative\n\nmodel = RNN(input_dim, embedding_dim, hidden_dim, n_layers, bidirectional, dropout, pad_idx, n_classes)","metadata":{"id":"4_4gdqbc77CI","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:30:25.540977Z","iopub.execute_input":"2025-08-30T03:30:25.541292Z","iopub.status.idle":"2025-08-30T03:30:25.632693Z","shell.execute_reply.started":"2025-08-30T03:30:25.541270Z","shell.execute_reply":"2025-08-30T03:30:25.632113Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"model.embedding.weight.data.copy_(word_embedding.vectors)\nmodel.embedding.weight.data[unk_idx] = torch.zeros(embedding_dim)\nmodel.embedding.weight.data[pad_idx] = torch.zeros(embedding_dim)","metadata":{"id":"syPlzxAlPq2f","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:30:29.459538Z","iopub.execute_input":"2025-08-30T03:30:29.460150Z","iopub.status.idle":"2025-08-30T03:30:29.469771Z","shell.execute_reply.started":"2025-08-30T03:30:29.460128Z","shell.execute_reply":"2025-08-30T03:30:29.469236Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","metadata":{"executionInfo":{"elapsed":40,"status":"ok","timestamp":1746194406406,"user":{"displayName":"Viet Tranhuu","userId":"08791169309344190178"},"user_tz":-420},"id":"dI2rLTSjP2Wf","outputId":"97310f26-c1da-48f7-b70f-385aca59b885","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:30:31.800868Z","iopub.execute_input":"2025-08-30T03:30:31.801182Z","iopub.status.idle":"2025-08-30T03:30:31.805849Z","shell.execute_reply.started":"2025-08-30T03:30:31.801160Z","shell.execute_reply":"2025-08-30T03:30:31.805021Z"}},"outputs":[{"name":"stdout","text":"The model has 10,004,123 trainable parameters\n","output_type":"stream"}],"execution_count":49},{"cell_type":"markdown","source":"## 4.6 Train the model","metadata":{"id":"-MF9RoIfP9ag"}},{"cell_type":"code","source":"import torch.optim as optim\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score","metadata":{"id":"tPTimkkcQFt0","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:30:34.642541Z","iopub.execute_input":"2025-08-30T03:30:34.643273Z","iopub.status.idle":"2025-08-30T03:30:34.646847Z","shell.execute_reply.started":"2025-08-30T03:30:34.643248Z","shell.execute_reply":"2025-08-30T03:30:34.646012Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\noptimizer = optim.Adam(model.parameters())\ncriterion = nn.CrossEntropyLoss().to(device) \n\nmodel = model.to(device)","metadata":{"id":"sSRqCavQP6aP","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:30:37.278465Z","iopub.execute_input":"2025-08-30T03:30:37.279194Z","iopub.status.idle":"2025-08-30T03:30:39.019142Z","shell.execute_reply.started":"2025-08-30T03:30:37.279172Z","shell.execute_reply":"2025-08-30T03:30:39.018333Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"def compute_metrics_1(preds, labels):\n    acc = accuracy_score(labels, preds)\n    return acc ","metadata":{"id":"ur6s3FBBQIb4","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:30:40.892540Z","iopub.execute_input":"2025-08-30T03:30:40.893460Z","iopub.status.idle":"2025-08-30T03:30:40.897109Z","shell.execute_reply.started":"2025-08-30T03:30:40.893435Z","shell.execute_reply":"2025-08-30T03:30:40.896445Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"def compute_metrics_2(preds, labels):\n    acc = accuracy_score(labels, preds)\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        labels, preds, average='weighted', zero_division=0\n    )\n    return acc, precision, recall, f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:30:51.072957Z","iopub.execute_input":"2025-08-30T03:30:51.073714Z","iopub.status.idle":"2025-08-30T03:30:51.077495Z","shell.execute_reply.started":"2025-08-30T03:30:51.073686Z","shell.execute_reply":"2025-08-30T03:30:51.076735Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"def train(model, dataloader, optimizer, criterion, device):\n    model.train()\n    epoch_loss = 0\n    all_preds, all_labels = [], []\n\n    for batch in dataloader:\n        optimizer.zero_grad()\n        reviews, lengths = batch['reviews']\n        reviews, lengths = reviews.to(device), lengths.to(device)\n        logits = model(reviews, lengths)\n        labels = batch['sentiments'].long().squeeze(-1).to(device)\n\n        loss = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n\n        preds = logits.argmax(dim=1).cpu().tolist()\n        all_preds.extend(preds)\n        all_labels.extend(labels.cpu().tolist())\n\n    acc = compute_metrics_1(all_preds, all_labels)\n    return epoch_loss / len(dataloader), acc ","metadata":{"id":"TSfvzJqKQM47","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:30:54.254011Z","iopub.execute_input":"2025-08-30T03:30:54.254654Z","iopub.status.idle":"2025-08-30T03:30:54.260605Z","shell.execute_reply.started":"2025-08-30T03:30:54.254630Z","shell.execute_reply":"2025-08-30T03:30:54.259763Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"def evaluate_1(model, dataloader, criterion, device):\n    model.eval()\n    epoch_loss = 0\n    all_preds, all_labels = [], []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            reviews, lengths = batch['reviews']\n            reviews, lengths = reviews.to(device), lengths.to(device)\n            logits = model(reviews, lengths)\n            labels = batch['sentiments'].long().squeeze(-1).to(device)\n\n            loss = criterion(logits, labels)\n            epoch_loss += loss.item()\n\n            preds = logits.argmax(dim=1).cpu().tolist()\n            all_preds.extend(preds)\n            all_labels.extend(labels.cpu().tolist())\n\n    acc = compute_metrics_1(all_preds, all_labels)\n    return epoch_loss / len(dataloader), acc","metadata":{"id":"7GOYp5OIQP6j","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:30:58.138072Z","iopub.execute_input":"2025-08-30T03:30:58.138673Z","iopub.status.idle":"2025-08-30T03:30:58.143683Z","shell.execute_reply.started":"2025-08-30T03:30:58.138652Z","shell.execute_reply":"2025-08-30T03:30:58.142928Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"def evaluate_2(model, dataloader, criterion, device):\n    model.eval()\n    epoch_loss = 0\n    all_preds, all_labels = [], []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            reviews, lengths = batch['reviews']\n            reviews, lengths = reviews.to(device), lengths.to(device)\n            logits = model(reviews, lengths)\n            labels = batch['sentiments'].long().squeeze(-1).to(device)\n\n            loss = criterion(logits, labels)\n            epoch_loss += loss.item()\n\n            preds = logits.argmax(dim=1).cpu().tolist()\n            all_preds.extend(preds)\n            all_labels.extend(labels.cpu().tolist())\n\n    acc, precision, recall, f1 = compute_metrics_2(all_preds, all_labels)\n    return epoch_loss / len(dataloader), acc, precision, recall, f1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:31:01.432063Z","iopub.execute_input":"2025-08-30T03:31:01.432786Z","iopub.status.idle":"2025-08-30T03:31:01.437859Z","shell.execute_reply.started":"2025-08-30T03:31:01.432764Z","shell.execute_reply":"2025-08-30T03:31:01.437156Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"import time","metadata":{"id":"3IbqwPJuQZ2d","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:31:05.595387Z","iopub.execute_input":"2025-08-30T03:31:05.596009Z","iopub.status.idle":"2025-08-30T03:31:05.599139Z","shell.execute_reply.started":"2025-08-30T03:31:05.595988Z","shell.execute_reply":"2025-08-30T03:31:05.598385Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"def epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","metadata":{"id":"h6CMYJsaQSpW","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:31:07.918886Z","iopub.execute_input":"2025-08-30T03:31:07.919461Z","iopub.status.idle":"2025-08-30T03:31:07.923381Z","shell.execute_reply.started":"2025-08-30T03:31:07.919437Z","shell.execute_reply":"2025-08-30T03:31:07.922570Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"n_epochs = 5\n\nbest_valid_loss = float(\"inf\")\n\nfor epoch in range(n_epochs):\n    start_time = time.time()\n\n    train_loss, train_acc = train(model, train_dataloader, optimizer, criterion, device)\n    valid_loss, valid_acc = evaluate_1(model, valid_dataloader, criterion, device)\n\n    end_time = time.time()\n\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n\n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'model.pt')\n\n    print(f\"Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s\")\n    print(f\"  Train - loss: {train_loss:.3f}| acc: {train_acc:.2f}\")\n    print(f\"  Valid - loss: {valid_loss:.3f}| acc: {valid_acc:.2f}\")","metadata":{"id":"zASLf1b3QcQi","outputId":"9d7d9954-7661-4a67-b0ac-32acfa4f6104","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:31:10.652337Z","iopub.execute_input":"2025-08-30T03:31:10.652587Z","iopub.status.idle":"2025-08-30T03:33:48.447932Z","shell.execute_reply.started":"2025-08-30T03:31:10.652570Z","shell.execute_reply":"2025-08-30T03:33:48.447018Z"}},"outputs":[{"name":"stdout","text":"Epoch: 01 | Time: 0m 31s\n  Train - loss: 0.711| acc: 0.71\n  Valid - loss: 0.443| acc: 0.84\nEpoch: 02 | Time: 0m 31s\n  Train - loss: 0.447| acc: 0.85\n  Valid - loss: 0.299| acc: 0.90\nEpoch: 03 | Time: 0m 31s\n  Train - loss: 0.336| acc: 0.90\n  Valid - loss: 0.234| acc: 0.93\nEpoch: 04 | Time: 0m 31s\n  Train - loss: 0.279| acc: 0.92\n  Valid - loss: 0.181| acc: 0.95\nEpoch: 05 | Time: 0m 31s\n  Train - loss: 0.239| acc: 0.93\n  Valid - loss: 0.153| acc: 0.96\n","output_type":"stream"}],"execution_count":59},{"cell_type":"markdown","source":"## 4.7 Test the model ","metadata":{}},{"cell_type":"code","source":"test_loss, test_acc, test_prec, test_rec, test_f1 = evaluate_2(model, test_dataloader, criterion, device)\n\nprint(f\"Test - loss: {test_loss:.3f}| acc: {test_acc:.2f}| prec: {test_prec:.2f}| rec: {test_rec:.2f}| f1: {test_f1:.2f}\")","metadata":{"id":"O1H6T20mUDgD","trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:33:54.328107Z","iopub.execute_input":"2025-08-30T03:33:54.328354Z","iopub.status.idle":"2025-08-30T03:33:55.461633Z","shell.execute_reply.started":"2025-08-30T03:33:54.328338Z","shell.execute_reply":"2025-08-30T03:33:55.460745Z"}},"outputs":[{"name":"stdout","text":"Test - loss: 0.655| acc: 0.83| prec: 0.85| rec: 0.83| f1: 0.84\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"import torch.nn.functional as F ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:34:00.430276Z","iopub.execute_input":"2025-08-30T03:34:00.430891Z","iopub.status.idle":"2025-08-30T03:34:00.434595Z","shell.execute_reply.started":"2025-08-30T03:34:00.430870Z","shell.execute_reply":"2025-08-30T03:34:00.433774Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"def predict_sentiment(model, sentence, vocab, device, label_mapping=None):\n    model.eval()\n\n    # Convert sentence to tensor of token indices\n    corpus = [sentence]\n    tensor = vocab.corpus_to_tensor(corpus)[0].to(device)        # [seq_len]\n    tensor = tensor.unsqueeze(1)                                 # [seq_len, 1]\n    length_tensor = torch.LongTensor([tensor.size(0)]).to(device)\n\n    # Forward pass\n    with torch.no_grad():\n        logits = model(tensor, length_tensor).squeeze(0)         # [n_classes]\n        probs = F.softmax(logits, dim=-1)                       # [n_classes]\n\n    # Predicted class index and optional label name\n    pred_idx = probs.argmax().item()\n    pred_label = label_mapping[pred_idx] if label_mapping is not None else str(pred_idx)\n\n    # Return index, label, and full probability distribution\n    return pred_label, probs.cpu().tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:34:03.234002Z","iopub.execute_input":"2025-08-30T03:34:03.234596Z","iopub.status.idle":"2025-08-30T03:34:03.239793Z","shell.execute_reply.started":"2025-08-30T03:34:03.234570Z","shell.execute_reply":"2025-08-30T03:34:03.238972Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"label_map = {0: 'tiêu cực', 1: 'bình thường', 2: 'tích cực'}\n\nwhile True:\n  input_text = input(\"Nhập câu cần kiểm tra: \")\n  if input_text.strip().lower() != \"thoát\":\n    label, probs = predict_sentiment(\n        model=model,\n        sentence=clean_text(input_text),\n        vocab=vocab,\n        device=device,\n        label_mapping=label_map\n    )\n    print(f\"\\nDự đoán cảm xúc: {label}\")\n  else:\n    print(\"Chúc một ngày tốt lành!\")\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-30T03:34:11.469044Z","iopub.execute_input":"2025-08-30T03:34:11.469655Z","iopub.status.idle":"2025-08-30T03:34:38.477864Z","shell.execute_reply.started":"2025-08-30T03:34:11.469634Z","shell.execute_reply":"2025-08-30T03:34:38.477240Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Nhập câu cần kiểm tra:  ổn\n"},{"name":"stdout","text":"Tokenize the corpus...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00, 3504.01it/s]\n100%|██████████| 1/1 [00:00<00:00, 6223.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nDự đoán cảm xúc: tích cực\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Nhập câu cần kiểm tra:  ok\n"},{"name":"stdout","text":"Tokenize the corpus...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00, 3809.54it/s]\n100%|██████████| 1/1 [00:00<00:00, 7463.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nDự đoán cảm xúc: tích cực\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Nhập câu cần kiểm tra:  cũng được\n"},{"name":"stdout","text":"Tokenize the corpus...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00, 2562.19it/s]\n100%|██████████| 1/1 [00:00<00:00, 6584.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nDự đoán cảm xúc: tích cực\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Nhập câu cần kiểm tra:  end\n"},{"name":"stdout","text":"Tokenize the corpus...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00, 3615.78it/s]\n100%|██████████| 1/1 [00:00<00:00, 5932.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nDự đoán cảm xúc: tích cực\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Nhập câu cần kiểm tra:  kết thúc\n"},{"name":"stdout","text":"Tokenize the corpus...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00, 2970.47it/s]\n100%|██████████| 1/1 [00:00<00:00, 6864.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nDự đoán cảm xúc: tích cực\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Nhập câu cần kiểm tra:  thoát\n"},{"name":"stdout","text":"Chúc một ngày tốt lành!\n","output_type":"stream"}],"execution_count":63}]}